{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw/IuimguBTcAks4oZm2nv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anupama83/PyTorch-for-Deep-Learning/blob/main/Downloading_video_files_from_YouTube_Converting_to_Audio_Remove_VAD_Chunk_NLTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdflqbJ5SZU4",
        "outputId": "bffad76e-9c5f-4152-cb6d-b859ab0226fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To download video files from YouTube\n",
        "## 1. Install and import important libraries\n"
      ],
      "metadata": {
        "id": "Qm2FuH_XUXWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL758iWpURUj",
        "outputId": "a217c2af-3022-4063-b5d4-72acd93416fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.1 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,199 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,652 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,513 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,552 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,854 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Fetched 19.6 MB in 3s (6,885 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 7,815 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 7,815 kB in 1s (5,683 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fvEZaLaUQ07",
        "outputId": "f3a48691-1348-415c-c566-983f40b75900"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTL7fXtuUnFF",
        "outputId": "6e452721-bd17-455a-e4ae-c8d179ce1d7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.12.23-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/172.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.12.23-py3-none-any.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/3.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2024.12.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "\n",
        "def download_with_ytdlp(url, output_path):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo+bestaudio',\n",
        "        'outtmpl': f'{output_path}/%(title)s.%(ext)s',\n",
        "        'merge_output_format': 'mp4',\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    print(\"Download completed.\")\n",
        "\n",
        "# Pass url of youtube video\n",
        "url = \"https://www.youtube.com/shorts/A3v9GrVxnNg\"\n",
        "# mention the path where you want to store your downloaded video file\n",
        "output_path = \"/content/drive/MyDrive/Research Work_Zero_Shot_Learning/Video Files for Dataset/Nirmala Sitaraman\"\n",
        "download_with_ytdlp(url, output_path)\n"
      ],
      "metadata": {
        "id": "kN9YaYZVUMRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting video file to audio file"
      ],
      "metadata": {
        "id": "621ikuaJUKQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import AudioFileClip\n",
        "import os\n",
        "\n",
        "def convert_mp4_to_wav(mp4_path, wav_path=None):\n",
        "    # Set the output path to the same directory with .wav extension if not provided\n",
        "    if wav_path is None:\n",
        "        wav_path = os.path.splitext(mp4_path)[0] + '.wav'\n",
        "\n",
        "    # Load the audio from the .mp4 file\n",
        "    with AudioFileClip(mp4_path) as audio_clip:\n",
        "        # Write the audio to .wav file\n",
        "        audio_clip.write_audiofile(wav_path, codec='pcm_s16le')\n",
        "\n",
        "    print(f\"Conversion complete. Audio saved as: {wav_path}\")\n",
        "\n",
        "# Pass the path of downloaded video file\n",
        "mp4_path = \"/content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/Venkaiah Naidu Last Speech In Parliament As Vice President I Was In Tears Didnt Ask For This.mp4\"  # Replace with the path to your .mp4 file\n",
        "convert_mp4_to_wav(mp4_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3i-XYhAU9oz",
        "outputId": "0b5adb07-eee6-42e8-e089-f9fac1402519"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/Venkaiah Naidu Last Speech In Parliament As Vice President I Was In Tears Didnt Ask For This.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Conversion complete. Audio saved as: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/Venkaiah Naidu Last Speech In Parliament As Vice President I Was In Tears Didnt Ask For This.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAD, Noise removing and Chunk the audio files"
      ],
      "metadata": {
        "id": "ZfTdNCv_VaOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install and Import Dependencies\n",
        "\n",
        "# this assumes that you have a relevant version of PyTorch installed\n",
        "!pip install -q torchaudio\n",
        "\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "from IPython.display import Audio\n",
        "from pprint import pprint\n",
        "# download example\n",
        "#torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')"
      ],
      "metadata": {
        "id": "58hFHKU8ZdnZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_PIP = True # download model using pip package or torch.hub\n",
        "USE_ONNX = False # change this to True if you want to test onnx model\n",
        "if USE_ONNX:\n",
        "    !pip install -q onnxruntime\n",
        "if USE_PIP:\n",
        "  !pip install -q silero-vad\n",
        "  from silero_vad import (load_silero_vad,\n",
        "                          read_audio,\n",
        "                          get_speech_timestamps,\n",
        "                          save_audio,\n",
        "                          VADIterator,\n",
        "                          collect_chunks)\n",
        "  model = load_silero_vad(onnx=USE_ONNX)\n",
        "else:\n",
        "  model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
        "                                model='silero_vad',\n",
        "                                force_reload=True,\n",
        "                                onnx=USE_ONNX)\n",
        "\n",
        "  (get_speech_timestamps,\n",
        "  save_audio,\n",
        "  read_audio,\n",
        "  VADIterator,\n",
        "  collect_chunks) = utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC3yZfW0Zdk0",
        "outputId": "52ff7aa1-5a72-4fac-cab9-b4a218228cf3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from pprint import pprint\n",
        "\n",
        "# Function to save speech chunks to a specified folder\n",
        "def save_audio_chunk(wav, start_time, end_time, output_folder, chunk_index):\n",
        "    # Convert start_time and end_time to sample indices\n",
        "    # Added error handling for invalid start_time and end_time\n",
        "    try:\n",
        "        start_sample = int(float(start_time) * SAMPLING_RATE)  # Convert to float first\n",
        "        end_sample = int(float(end_time) * SAMPLING_RATE)  # Convert to float first\n",
        "    except ValueError:\n",
        "        print(f\"Skipping chunk {chunk_index} due to invalid start_time or end_time: start_time={start_time}, end_time={end_time}\")\n",
        "        return  # Skip this chunk\n",
        "\n",
        "    # Extract the chunk\n",
        "    chunk = wav[:, start_sample:end_sample]\n",
        "\n",
        "    # Define the output path for the chunk\n",
        "    output_path = os.path.join(output_folder, f\"chunk_{chunk_index}.wav\")\n",
        "\n",
        "    # Save the chunk as a WAV file\n",
        "    torchaudio.save(output_path, chunk, SAMPLING_RATE)\n",
        "    print(f\"Saved chunk {chunk_index} to {output_path}\")\n",
        "\n",
        "# Function to process the audio, chunk it, and save the chunks\n",
        "def process_audio_and_save_chunks(input_audio_path, output_folder, model, sampling_rate=16000):\n",
        "    # Load the full audio file\n",
        "    wav, _ = torchaudio.load(input_audio_path, normalize=True)\n",
        "\n",
        "    # Check if audio has more than one channel and convert to mono if necessary\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)  # Convert to mono by averaging channels\n",
        "\n",
        "    # Get speech timestamps (assumes get_speech_timestamps is defined and model is loaded)\n",
        "    speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)\n",
        "    pprint(speech_timestamps)  # Optionally print the timestamps\n",
        "\n",
        "    # Create the output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate over the speech timestamps and save each chunk\n",
        "    for chunk_index, (start_time, end_time) in enumerate(speech_timestamps):\n",
        "        save_audio_chunk(wav, start_time, end_time, output_folder, chunk_index)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_audio_path = '/content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/Venkaiah Naidu Last Speech In Parliament As Vice President I Was In Tears Didnt Ask For This.wav'\n",
        "output_folder = '/content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/'\n",
        "SAMPLING_RATE = 16000  # Define your sampling rate\n",
        "# Make sure to load your model here\n",
        "model = load_silero_vad()  # Assuming you are using silero-vad\n",
        "\n",
        "\n",
        "# Call the process function\n",
        "process_audio_and_save_chunks(input_audio_path, output_folder, model, sampling_rate=SAMPLING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwvHJ2pOU9d7",
        "outputId": "daa49dea-ec71-444e-8cec-b1fe5cbd45fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'end': 57824, 'start': 4640}, {'end': 66016, 'start': 59936}]\n",
            "Skipping chunk 0 due to invalid start_time or end_time: start_time=start, end_time=end\n",
            "Skipping chunk 1 due to invalid start_time or end_time: start_time=start, end_time=end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miSbPcwvU9Yj",
        "outputId": "42105e2b-b6e2-45c8-c7b1-f9bd89f7ff56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def split_wav_file(input_file, output_dir, chunk_duration=20):\n",
        "    \"\"\"\n",
        "    Splits a .wav file into chunks of a specified duration.\n",
        "\n",
        "    :param input_file: Path to the input .wav file.\n",
        "    :param output_dir: Directory to save the chunks.\n",
        "    :param chunk_duration: Duration of each chunk in seconds (default is 20 seconds).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the .wav file\n",
        "        audio = AudioSegment.from_wav(input_file)\n",
        "        total_duration = len(audio)  # Total duration in milliseconds\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Split the audio into chunks\n",
        "        chunk_duration_ms = chunk_duration * 1000  # Convert to milliseconds\n",
        "        for i in range(0, total_duration, chunk_duration_ms):\n",
        "            chunk = audio[i:i + chunk_duration_ms]\n",
        "            chunk_name = f\"chunk_{i // chunk_duration_ms + 1}.wav\"\n",
        "            chunk_path = os.path.join(output_dir, chunk_name)\n",
        "            chunk.export(chunk_path, format=\"wav\")\n",
        "            print(f\"Exported: {chunk_path}\")\n",
        "\n",
        "        print(\"Splitting complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "input_wav = \"/content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/Venkaiah Naidu Last Speech In Parliament As Vice President I Was In Tears Didnt Ask For This.wav\"\n",
        "output_directory = \"/content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar\" # Create a folder with speaker name and pass that path here\n",
        "split_wav_file(input_wav, output_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FX6I3MeaShm",
        "outputId": "4c6fe2aa-1d37-4a17-c9a8-777d6af7f4aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_1.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_2.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_3.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_4.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_5.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_6.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_7.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_8.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_9.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_10.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_11.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_12.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_13.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_14.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_15.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_16.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_17.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_18.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_19.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_20.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_21.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_22.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_23.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_24.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_25.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_26.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_27.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_28.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_29.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_30.wav\n",
            "Exported: /content/drive/MyDrive/Research Work_Speaker Recognition/Dataset videos/chunked files_Jagdeep_Dhankar/Jagdeep Dhankar/chunk_31.wav\n",
            "Splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNDNsz4TbUrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}